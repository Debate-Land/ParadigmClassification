{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d9ae422607a4db7880110078351b7a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/Users/sam/.cache/huggingface/datasets/debate-land___csv/debate-land--2023-paradigms-4c55cafa927485ac/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ca85bddbae04e73ae5344158d7c48fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "paradigms = load_dataset(\"debate-land/2023-paradigms\")[\"train\"].train_test_split(test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Email: shoxha2020@gmail.com\\nIntro / About Me:\\nShout out to Westside High and UH - I wouldn\\'t be anywhere without you. <3\\nDon\\'t be discriminatory. I\\'m warning you now if you have to ask, \"Is this problematic? Don\\'t read it - there are better strategies out there.\\nAlso Important: If you read spreading bad in front of me, I will not hack for you. I can spread and I can flow, but I am disabled and these skills were harder for me to develop than most. Many debaters see this as an opportunity for a persuasive 2ar and 2nr push, don\\'t let this be you. I consider this motivated and ableist.\\nYou\\'re either winning an argument on the flow or you\\'re not. Trivializing my struggles or the struggles of any judge for the ballot is an easy way to get me to despise you.\\nDebate is a game, but it is an academic game. Tech over truth, but truth constrains tech. You\\'ll have a harder time convincing me global warming is fake than convincing me warming will destroy the planet. If two debaters are equal on a particular flow, truth is the obvious tie breaker.\\nI will try to intervene as little as possible - I\\'m old school in that you need to explain things to me like I\\'m 5 for me to grant you the arguments you want to go for.\\nI have been in this space for too long. I have zero clue how some old heads have been here for 20+ years. As such, it\\'s becoming much harder to tolerate cringe, posturing, flexing, and generally being an obnoxious debater stereotype. While I will not punish you for it, it will still make me cringe. Be nice to people, there\\'s a difference between being confident and being mean.\\nI vibe check speaks, I don\\'t know what a 30 looks like, but I can feel it. But that doesn\\'t mean that speaks are arbitrary because my flow checks my vibes. I default to a 28.5 and go higher or lower based on your strategic decisions.\\nOnline debate and its consequences have been a disaster for the debate community. Disclose quickly, don\\'t steal prep. I am growing tired of people that can\\'t manage their files and make a 45-minute round an hour long.\\nPost UT update: Post rounding is cool and checks against dumb decisions, I frequently make bad decisions and I encourage you ask questions, but do it nicely.\\nNow for the gross stuff\\n\\nK\\nI love the K. I\\'ve read many lit bases.\\n\\nKnow your lit, theorize, and don\\'t neglect the material implications of your literature.\\n\\nI think generic links are fine, but specific links are always better. Saying that a K link is generic and so I should gut check it is never sufficient - you need to explain why a generic link doesn\\'t apply to your aff.\\n\\nDon\\'t drop your alt unless you\\'re winning a framework push because dropping the alt means that I have to weigh the aff versus the status quo, and 9 out of 10, you will lose that debate.\\n\\nI default to weighing the aff against the K or something to that effect. If you wan\\'t me to exclude aff offense, you need to do some heavy work.\\n\\nFairness is not a good argument if a K team is winning that your model is problematic, justify policy making and then cry about fairness.\\n\\nSubstantive reasons for why they don\\'t get the perm > Theoretical reasons for why they don\\'t get the perm.\\n\\nYou must explain how the perm works for me and the net benefit. Saying \"perm do both\" - is okay but super weak and usually will not be enough to overcome disads to the perm.\\n\\nK Affs\\nLove kritikal affs, but TVAs usually pick up my ballot here. You need to explain your model of debate / method. You should have a strong relationship to the topic or at least explain why a relationship to the topic is bad or doesn\\'t matter.\\n\\nFramework:\\nDefine how your method of debate works, the benefits only your method can access, and why you can include their model / arguments, even if they can\\'t argue for their perfect advocacy.\\nGenerally speaking, it\\'s okay if the topic excludes your specific author - you don\\'t get the perfect aff sometimes, it is what it is. Debate is about controversies and every advocacy (mostly) will have side-constraints, disadvantages, or criticism from different schools of thought. You should embrace this.\\nDon\\'t neglect case - if they\\'re winning that their scholarship is good and key, it\\'ll be much harder for you to win this flow.\\nT\\nDebatability is not the sole metric that I use to decide T debates. Real world application of literature is another side-constraint of an interpretation.\\n\\nSure, your interpretation might produce the most clash, but if there\\'s no exportable topic education, what\\'s the point of clash?\\n\\nI\\'m very happy to vote on \"Nobody in X field or expertise defines the words in the resolution in a specific way.\" I hate fake debate T interpretations with 0 real world application.\\n\\nYou need to weigh between standards and different implications of interpretations.\\n\\nAlso weigh definitions - but saying, \"Our definition is from a reliable source, and yours isn\\'t.\" is not an argument.\\n\\nCompeting Interps > Reasonability.\\n\\nPolicy\\nDeploy whatever arguments you need to win the round.\\nI love a good counterplan gimmick.\\nPics are good. But my default can change.\\nDelay counterplans are not legit. Unless, the net benefit is fire and super specific.\\nProcess counterplans are suspect, but I\\'m willing to vote on them.\\nActor counterplans are fine.\\nYou must justify judge kick - and say you\\'re kicking something.\\nUse differential degrees or lense of sufficiency framing to explain how I should evaluate solvency deficits vs. the net benefit of the counterplan.\\nWeigh between different scenarios please.\\nCompare warrants and explain why yours are better, this is super neglected in policy and LD especially.\\nExplain how the PIC solves the aff. I will not give this to you just because you label something a pic.\\nNo opinions on condo, dispo, or how many offs are too much. I will police this more in LD. I think 2 to 3 condo positions + squo is enough neg flex, but you\\'re more than welcome to convince me otherwise. I really don\\'t care.\\nThere can be 0 risk of a DA - but it\\'s very rare. You need to do stellar work here for me to say there\\'s no risk to the DA.\\n\\nTheory: 3\\nI don\\'t like these debates in LD - they\\'re way overused.\\nIn policy, theory debates are fine.\\nDefaults:\\nReasonabilty > Competing Interps\\nNo RVIs.\\nYes, 1AR theory.\\nDTA > DTD, unless DTA is impossible.\\n\\nTricks:\\nI used to discriminate against these arguments, but there\\'s no reason why these arguments are any less legit than the K, a DA, or T. I\\'m just not qualified to be your judge - read at your own risk.',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paradigms['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ea8a0967f3b4af7bbb3b9c68e856b10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/343 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f69c3c3a568471abc27b27915fb0f2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/148 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_paradigms = paradigms.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: \"FLOW\",\n",
    "    1: \"FLAY\",\n",
    "    2: \"LAY\"\n",
    "}\n",
    "\n",
    "label2id = {\n",
    "    \"FLOW\": 0,\n",
    "    \"FLAY\": 1,\n",
    "    \"LAY\": 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import create_optimizer\n",
    "import tensorflow as tf\n",
    "\n",
    "batch_size = 16\n",
    "num_epochs = 5\n",
    "batches_per_epoch = len(tokenized_paradigms[\"train\"]) // batch_size\n",
    "total_train_steps = int(batches_per_epoch * num_epochs)\n",
    "#optimizer = tf.keras.optimizers.legacy.Adam()\n",
    "optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=3, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "tf_train_set: tf.data.Dataset = model.prepare_tf_dataset(\n",
    "    tokenized_paradigms[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_validation_set = model.prepare_tf_dataset(\n",
    "    tokenized_paradigms[\"test\"],\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.keras_callbacks import KerasMetricCallback\n",
    "\n",
    "metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning http-samc/paradigm-model into local empty directory.\n",
      "WARNING:huggingface_hub.repository:Cloning http-samc/paradigm-model into local empty directory.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "WARNING: `git lfs clone` is deprecated and will not be updated\n          with new flags from `git clone`\n\n`git clone` has been updated in upstream Git to have comparable\nspeeds to `git lfs clone`.\nfatal: repository 'http-samc/paradigm-model' does not exist\nError(s) during clone:\n`git clone` failed: exit status 128\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/dev/ParadigmClassification/env/lib/python3.10/site-packages/huggingface_hub/repository.py:669\u001b[0m, in \u001b[0;36mRepository.clone_from\u001b[0;34m(self, repo_url, token)\u001b[0m\n\u001b[1;32m    667\u001b[0m             env\u001b[39m.\u001b[39mupdate({\u001b[39m\"\u001b[39m\u001b[39mGIT_LFS_SKIP_SMUDGE\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m1\u001b[39m\u001b[39m\"\u001b[39m})\n\u001b[0;32m--> 669\u001b[0m         run_subprocess(\n\u001b[1;32m    670\u001b[0m             \u001b[39m# 'git lfs clone' is deprecated (will display a warning in the terminal)\u001b[39;49;00m\n\u001b[1;32m    671\u001b[0m             \u001b[39m# but we still use it as it provides a nicer UX when downloading large\u001b[39;49;00m\n\u001b[1;32m    672\u001b[0m             \u001b[39m# files (shows progress).\u001b[39;49;00m\n\u001b[1;32m    673\u001b[0m             \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39m'\u001b[39;49m\u001b[39mgit clone\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39mif\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mskip_lfs_files\u001b[39m \u001b[39;49m\u001b[39melse\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mgit lfs clone\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m{\u001b[39;49;00mrepo_url\u001b[39m}\u001b[39;49;00m\u001b[39m .\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    674\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlocal_dir,\n\u001b[1;32m    675\u001b[0m             env\u001b[39m=\u001b[39;49menv,\n\u001b[1;32m    676\u001b[0m         )\n\u001b[1;32m    677\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    678\u001b[0m     \u001b[39m# Check if the folder is the root of a git repository\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/ParadigmClassification/env/lib/python3.10/site-packages/huggingface_hub/utils/_subprocess.py:83\u001b[0m, in \u001b[0;36mrun_subprocess\u001b[0;34m(command, folder, check, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m     folder \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(folder)\n\u001b[0;32m---> 83\u001b[0m \u001b[39mreturn\u001b[39;00m subprocess\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m     84\u001b[0m     command,\n\u001b[1;32m     85\u001b[0m     stderr\u001b[39m=\u001b[39;49msubprocess\u001b[39m.\u001b[39;49mPIPE,\n\u001b[1;32m     86\u001b[0m     stdout\u001b[39m=\u001b[39;49msubprocess\u001b[39m.\u001b[39;49mPIPE,\n\u001b[1;32m     87\u001b[0m     check\u001b[39m=\u001b[39;49mcheck,\n\u001b[1;32m     88\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     89\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mreplace\u001b[39;49m\u001b[39m\"\u001b[39;49m,  \u001b[39m# if not utf-8, replace char by ï¿½\u001b[39;49;00m\n\u001b[1;32m     90\u001b[0m     cwd\u001b[39m=\u001b[39;49mfolder \u001b[39mor\u001b[39;49;00m os\u001b[39m.\u001b[39;49mgetcwd(),\n\u001b[1;32m     91\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m     92\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/subprocess.py:526\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    525\u001b[0m     \u001b[39mif\u001b[39;00m check \u001b[39mand\u001b[39;00m retcode:\n\u001b[0;32m--> 526\u001b[0m         \u001b[39mraise\u001b[39;00m CalledProcessError(retcode, process\u001b[39m.\u001b[39margs,\n\u001b[1;32m    527\u001b[0m                                  output\u001b[39m=\u001b[39mstdout, stderr\u001b[39m=\u001b[39mstderr)\n\u001b[1;32m    528\u001b[0m \u001b[39mreturn\u001b[39;00m CompletedProcess(process\u001b[39m.\u001b[39margs, retcode, stdout, stderr)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['git', 'lfs', 'clone', 'http-samc/paradigm-model', '.']' returned non-zero exit status 2.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras_callbacks\u001b[39;00m \u001b[39mimport\u001b[39;00m PushToHubCallback\n\u001b[0;32m----> 3\u001b[0m push_to_hub_callback \u001b[39m=\u001b[39m PushToHubCallback(\n\u001b[1;32m      4\u001b[0m     output_dir\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mparadigm-model\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      5\u001b[0m     tokenizer\u001b[39m=\u001b[39;49mtokenizer,\n\u001b[1;32m      6\u001b[0m )\n",
      "File \u001b[0;32m~/dev/ParadigmClassification/env/lib/python3.10/site-packages/transformers/keras_callbacks.py:344\u001b[0m, in \u001b[0;36mPushToHubCallback.__init__\u001b[0;34m(self, output_dir, save_strategy, save_steps, tokenizer, hub_model_id, hub_token, checkpoint, **model_card_args)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhub_model_id \u001b[39m=\u001b[39m hub_model_id\n\u001b[1;32m    343\u001b[0m create_repo(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhub_model_id, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 344\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrepo \u001b[39m=\u001b[39m Repository(\u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_dir), clone_from\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhub_model_id, token\u001b[39m=\u001b[39;49mhub_token)\n\u001b[1;32m    346\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer \u001b[39m=\u001b[39m tokenizer\n\u001b[1;32m    347\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_job \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/ParadigmClassification/env/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/dev/ParadigmClassification/env/lib/python3.10/site-packages/huggingface_hub/repository.py:516\u001b[0m, in \u001b[0;36mRepository.__init__\u001b[0;34m(self, local_dir, clone_from, repo_type, token, git_user, git_email, revision, skip_lfs_files, client)\u001b[0m\n\u001b[1;32m    513\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhuggingface_token \u001b[39m=\u001b[39m HfFolder\u001b[39m.\u001b[39mget_token()\n\u001b[1;32m    515\u001b[0m \u001b[39mif\u001b[39;00m clone_from \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 516\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclone_from(repo_url\u001b[39m=\u001b[39;49mclone_from)\n\u001b[1;32m    517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    518\u001b[0m     \u001b[39mif\u001b[39;00m is_git_repo(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlocal_dir):\n",
      "File \u001b[0;32m~/dev/ParadigmClassification/env/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/dev/ParadigmClassification/env/lib/python3.10/site-packages/huggingface_hub/repository.py:709\u001b[0m, in \u001b[0;36mRepository.clone_from\u001b[0;34m(self, repo_url, token)\u001b[0m\n\u001b[1;32m    706\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(error_msg)\n\u001b[1;32m    708\u001b[0m \u001b[39mexcept\u001b[39;00m subprocess\u001b[39m.\u001b[39mCalledProcessError \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m--> 709\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(exc\u001b[39m.\u001b[39mstderr)\n",
      "\u001b[0;31mOSError\u001b[0m: WARNING: `git lfs clone` is deprecated and will not be updated\n          with new flags from `git clone`\n\n`git clone` has been updated in upstream Git to have comparable\nspeeds to `git lfs clone`.\nfatal: repository 'http-samc/paradigm-model' does not exist\nError(s) during clone:\n`git clone` failed: exit status 128\n"
     ]
    }
   ],
   "source": [
    "from transformers.keras_callbacks import PushToHubCallback\n",
    "\n",
    "push_to_hub_callback = PushToHubCallback(\n",
    "    output_dir=\"paradigm-model\",\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [metric_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model.compile(optimizer=optimizer)  # No loss argument!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x29968e8c0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/assets\n"
     ]
    }
   ],
   "source": [
    "model.export(filepath='model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LAY'"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "I will vote with what's on what is on the flow only. I enter the round tabula rasa, i try to check my personal opinions at the door as best as i can. I may mock you for it, but I wonâ€™t vote against you for it. No paraphrasing. Quote the author, date and the exact words. Quals are even better but you donâ€™t have to read them unless pressed. Have the website handy. Research is critical.\n",
    "\n",
    "Speed? Meh. You cannot possibly go fast enough for me to not be able to follow you. However, that does not mean I want to hear you go fast. You can be quick and very persuasive. You don't need to spread.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "inputs = tokenizer(text, return_tensors=\"tf\")\n",
    "\n",
    "logits = model(**inputs).logits\n",
    "\n",
    "predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])\n",
    "model.config.id2label[predicted_class_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.math.argmax(logits, axis=-1)[0]\n",
    "print(\"!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(save_directory='model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
